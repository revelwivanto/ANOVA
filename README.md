# Analysis of Variance (ANOVA)

## Table of Contents
1. [Introduction](#introduction)  
2. [Why Use ANOVA?](#why-use-anova)  
3. [Types of ANOVA](#types-of-anova)  
4. [Key Concepts & Terminology](#key-concepts--terminology)  
5. [Statistical Model & Hypotheses](#statistical-model--hypotheses)  
6. [Assumptions](#assumptions)  
7. [ANOVA Table & Formulas](#anova-table--formulas)  
8. [Step-by-Step Example](#step-by-step-example)  
9. [Implementations](#implementations)  
   - [Python (SciPy)](#python-scipy)  
   - [Excel](#excel)  
10. [Interpreting Results](#interpreting-results)  
11. [Limitations & Alternatives](#limitations--alternatives)  
12. [References](#references)  

---

## Introduction  
Analysis of Variance (ANOVA) is a collection of statistical models and their associated estimation procedures used to analyze the differences among group means. Invented by Ronald A. Fisher in the 1920s, ANOVA extends the two-sample _t_-test to more than two groups by partitioning variability in the data into components.

---

## Why Use ANOVA?  
- **Multiple Groups**: When comparing means across three or more independent samples.  
- **Control Type I Error**: Conducting multiple _t_-tests inflates the probability of a false positive; ANOVA controls the overall significance level.  
- **Decompose Variance**: Separates total variability into ‚Äúbetween‚Äêgroup‚Äù and ‚Äúwithin‚Äêgroup‚Äù components, helping to pinpoint sources of variation.

---

## Types of ANOVA  
| Type             | Design                           | Notes                                                  |
|------------------|----------------------------------|--------------------------------------------------------|
| **One-Way ANOVA**| One categorical factor           | Tests whether ùúá‚ÇÅ = ùúá‚ÇÇ = ‚Ä¶ = ùúá‚Çñ for _k_ groups.         |
| **Two-Way ANOVA**| Two categorical factors          | Can test main effects and interaction effect.          |
| **Repeated Measures ANOVA** | Same subjects at multiple levels | Accounts for correlation between repeated observations. |
| **Mixed-Effects ANOVA** | Fixed and random effects         | Models both population‚Äêlevel and subject‚Äêlevel effects. |

---

## Key Concepts & Terminology  
- **Group Mean (ùë•ÃÑ·µ¢)**: Average of observations in group _i_.  
- **Grand Mean (ùë•ÃÑ)**: Average across all observations (all groups).  
- **Between‚ÄêGroup Sum of Squares (SS<sub>B</sub>)**:  
  \[
    SS_B = \sum_{i=1}^{k} n_i (xÃÑ_i - xÃÑ)^2
  \]  
- **Within‚ÄêGroup Sum of Squares (SS<sub>W</sub>)**:  
  \[
    SS_W = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (x_{ij} - xÃÑ_i)^2
  \]  
- **Total Sum of Squares (SS<sub>T</sub>)**:  
  \[
    SS_T = SS_B + SS_W = \sum_{i=1}^{k} \sum_{j=1}^{n_i} ( x_{ij} - xÃÑ )^2
  \]  
- **Degrees of Freedom (df)**:  
  - Between: _k‚Äì1_  
  - Within: _N‚Äìk_  
  - Total: _N‚Äì1_  
- **Mean Squares (MS)**: SS divided by corresponding df.  
  - MS<sub>B</sub> = SS<sub>B</sub> / (k‚Äì1)  
  - MS<sub>W</sub> = SS<sub>W</sub> / (N‚Äìk)  
- **F-Statistic**:  
  \[
    F = \frac{MS_B}{MS_W}
  \]

---

## Statistical Model & Hypotheses  
For a one‚Äêway ANOVA with _k_ groups and observations _x<sub>ij</sub>_:

\[
  x_{ij} = \mu + \tau_i + \varepsilon_{ij}, \quad \varepsilon_{ij}\sim N(0,\sigma^2)
\]

- **Null Hypothesis (H‚ÇÄ)**: All group means are equal:  
  \[
    H_0:\; \mu_1 = \mu_2 = \dots = \mu_k
  \]
- **Alternative Hypothesis (H‚ÇÅ)**: At least one group mean differs.

---

## Assumptions  
1. **Independence**: Observations are independent.  
2. **Normality**: Residuals _Œµ<sub>ij</sub>_ are normally distributed.  
3. **Homoscedasticity**: Equal variances across groups (œÉ¬≤ common).  

> _Note_: If homoscedasticity fails, consider Welch‚Äôs ANOVA or non‚Äêparametric Kruskal‚ÄìWallis test.

---

## ANOVA Table & Formulas  

| Source        | SS         | df     | MS              | F               |
|---------------|------------|--------|-----------------|-----------------|
| Between (B)   | SS<sub>B</sub> | k‚Äì1    | MS<sub>B</sub>=SS<sub>B</sub>/(k‚Äì1) | F=MS<sub>B</sub>/MS<sub>W</sub> |
| Within (W)    | SS<sub>W</sub> | N‚Äìk    | MS<sub>W</sub>=SS<sub>W</sub>/(N‚Äìk) |                 |
| **Total (T)** | SS<sub>T</sub> | N‚Äì1    |                 |                 |

- **Critical F-value**: _F<sub>crit</sub> = F<sub>Œ±;‚Äâdf1=k‚Äì1;‚Äâdf2=N‚Äìk</sub>_  
- **P-value**: _P = P(F ‚â• observed F)_

---

## Step-by-Step Example  
Suppose three fertilizers (A, B, C) tested on crop yield (kg) over _n = 5_ plots each:

| Plot | A   | B   | C   |
|------|-----|-----|-----|
| 1    | 30  | 28  | 35  |
| 2    | 32  | 26  | 33  |
| 3    | 31  | 27  | 36  |
| 4    | 29  | 25  | 34  |
| 5    | 33  | 29  | 37  |

1. **Compute group means**:  
   - ùë•ÃÑ<sub>A</sub> = 31, ùë•ÃÑ<sub>B</sub> = 27, ùë•ÃÑ<sub>C</sub> = 35  
   - Grand mean ùë•ÃÑ = (31+27+35)/3 = 31  

2. **Compute SS<sub>B</sub>**:  
   \[
     SS_B = 5[(31-31)^2 + (27-31)^2 + (35-31)^2]
           = 5[0 + 16 + 16] = 160
   \]

3. **Compute SS<sub>W</sub>**:  
   \[
     SS_W = \sum_i \sum_j (x_{ij} - xÃÑ_i)^2
           = \ldots = 40  \quad(\text{details omitted for brevity})
   \]

4. **Degrees of freedom**:  
   - df<sub>B</sub> = 3‚Äì1 = 2  
   - df<sub>W</sub> = 15‚Äì3 = 12  

5. **Mean squares**:  
   - MS<sub>B</sub> = 160 / 2 = 80  
   - MS<sub>W</sub> = 40 / 12 ‚âà 3.333  

6. **F-statistic**:  
   \[
     F = 80 / 3.333 ‚âà 24
   \]

7. **Decision**:  
   - For Œ± = 0.05, df1 = 2, df2 = 12, _F<sub>crit</sub>_ ‚âà 3.89.  
   - Observed F (24) > 3.89 ‚áí reject H‚ÇÄ ‚áí at least one mean differs.

---

## Implementations

### Python (SciPy)
```python
import numpy as np
from scipy import stats

# Data
A = np.array([30, 32, 31, 29, 33])
B = np.array([28, 26, 27, 25, 29])
C = np.array([35, 33, 36, 34, 37])

F_stat, p_value = stats.f_oneway(A, B, C)
print(f"F = {F_stat:.3f}, p = {p_value:.3f}")
